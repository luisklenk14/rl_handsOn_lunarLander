# rl_handsOn_lunarLander
 Landen Lernen: Entwicklung eines Reinforcement-Learning-Agenten in der LunarLander-Umgebung


# ğŸ›°ï¸ Reinforcement Learning mit PPO in der Umgebung `LunarLander-v3`

## ğŸ” ProjektÃ¼berblick

Dieses Projekt untersucht die **Entwicklung, das Verhalten und die Performance** eines Reinforcement-Learning-Agenten auf Basis des **Proximal Policy Optimization (PPO)**-Algorithmus in der bekannten **OpenAI Gymnasium-Umgebung `LunarLander-v3`**.

Ziel war es, einen Agenten zu trainieren, der lernt, ein Landemodul kontrolliert und sicher auf einer Plattform zu landen â€” ein klassisches Beispiel fÃ¼r **kontinuierliche Steuerung unter Unsicherheit** in einem **Markov Decision Process (MDP)**.

---

## ğŸ¯ Zielsetzung

* Training eines RL-Agenten mit PPO zur LÃ¶sung der LunarLander-Aufgabe
* Analyse des Trainingsverlaufs durch geeignete Metriken und Visualisierungen
* Visuelle Evaluation des Agentenverhaltens anhand von gespeicherten Videos
* Kritische Reflexion des Lernverhaltens, der Belohnungsstruktur und mÃ¶glicher Verbesserungen

---

## âš™ï¸ Setup & AusfÃ¼hrung

### ğŸ§± Voraussetzungen

* Python â‰¥ 3.8
* AbhÃ¤ngigkeiten sind in requirements.txt definiert
* pip install -r requirements.txt

---

### â–¶ï¸ Training starten

Das Skript trainiert einen PPO-Agenten fÃ¼r 500.000 Schritte mit Zwischenevaluationen.

---

### ğŸ“Š Evaluierung & Visualisierung

Nach dem Training werden zwei Diagramme erzeugt:

* **Reward-Verlauf** Ã¼ber die Zeit (Lernkurve + gleitender Mittelwert)
* **EpisodenlÃ¤nge** Ã¼ber die Zeit (zeigt Effizienz des Agenten)

ZusÃ¤tzlich wurden seperat drei Evaluierungsvideos erzeugt und gespeichert:

* Nach **20**, **10.000** und **500.000** Schritten
* Dienen zur qualitativen EinschÃ¤tzung des Fortschritts

---

## ğŸ“ˆ Ergebnisse & Beobachtungen

| Timestep  | Beobachtung (Video)                           |
| --------- | --------------------------------------------- |
| 20        | Agent stÃ¼rzt unkontrolliert ab                |
| 10.000    | Agent zeigt Steuerung, landet aber unprÃ¤zise  |
| 500.000 | Agent landet kontrolliert, aber knapp im Ziel |

### ğŸ“‰ Lernverlauf

* Deutlich steigende Rewards bis ca. **200.000** Schritte â†’ schneller Lernerfolg
* **EpisodenlÃ¤nge** sinkt im Verlauf â†’ Agent findet effizientere Strategien
* Belohnungsstruktur belohnt Landung, aber nicht PrÃ¤zision â†’ erklÃ¤rt Limitierung

---

## ğŸ§  Kritische Reflexion

* PPO ist robust, aber nicht optimal fÃ¼r **Feinsteuerung**
* Belohnungsfunktion der Umgebung kann zu â€suboptimalem Verhaltenâ€œ fÃ¼hren
* Videos sind unerlÃ¤sslich zur **qualitativen Bewertung**
* Reward allein spiegelt nicht immer reale SteuerungsqualitÃ¤t wider

---

## ğŸ”® MÃ¶gliche Erweiterungen

* Alternative Algorithmen: DDPG, TD3, SAC fÃ¼r kontinuierliche Kontrolle
* Reward-Shaping zur FÃ¶rderung von PrÃ¤zision
* Curriculum Learning oder Transfer Learning
* Integration eines Custom Reward-Trackings

---

## ğŸ“ Projektstruktur

```
â”œâ”€â”€ training_lunar.ipynb      # Training + Speicherung Agent evaluieren + Video erzeugen
â”œâ”€â”€ models/                   # Gespeicherte Modelle (beste & letzte)
â”œâ”€â”€ logs/                     # TensorBoard Logs & Evaluationsergebnisse
â”œâ”€â”€ videos/                   # Evaluation als Video (MP4)
â””â”€â”€ README.md                 # Diese Datei
```

---

## âœï¸ Motivation

Dieses Projekt entstand im Rahmen eines Hochschulprojekts zur **praktischen Anwendung von Reinforcement Learning**. 
Die Simulation vermittelt spielerisch, aber herausfordernd die Grundprinzipien der autonomen Entscheidungsfindung. Als jemand mit Interesse an Computerspielen und Robotik war dies eine motivierende und anschauliche Wahl. AuÃŸerdem hÃ¤tte ich jetzt noch eine Drohne mehr, wenn sie es geschafft hÃ¤tte dort zu landen, wo sie gestartet ist.

